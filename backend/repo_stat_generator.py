#!/usr/bin/env python3
"""
repo_stat_generator.py: Generate detailed statistics from repository analysis.

This script uses the data generated by repo_analyzer.py to create a comprehensive
statistical report with insights into repository activity patterns, commit behaviors,
and project development patterns.

Usage:
    python repo_stat_generator.py --stats /path/to/repo_stats.json
"""

import argparse
import json
import os
import datetime
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import numpy as np
import re
from dateutil.parser import parse

class StatGenerator:
    def __init__(self, stats_file):
        """Initialize with the path to the stats JSON file."""
        self.stats_file = stats_file
        self.stats = self.load_stats(stats_file)
        self.processed_stats = {}
        
    def load_stats(self, stats_file):
        """Load statistics from a JSON file."""
        with open(stats_file, 'r') as f:
            stats = json.load(f)
        
        # Convert string dates back to datetime objects where needed
        self.convert_string_dates(stats)
        
        return stats
    
    def convert_string_dates(self, stats):
        """Convert string dates in stats dictionary to datetime objects."""
        # Convert dates in commit data
        for repo, commits in stats['commit_data'].items():
            for commit in commits:
                try:
                    commit['author_date'] = parse(commit['author_date'])
                    commit['commit_date'] = parse(commit['commit_date'])
                except:
                    pass
        
        # Convert dates in summary stats
        for repo, repo_stats in stats['summary_stats'].items():
            for key in ['first_commit', 'last_commit']:
                if key in repo_stats and isinstance(repo_stats[key], str):
                    try:
                        repo_stats[key] = parse(repo_stats[key])
                    except:
                        pass
            
            # Convert time_span from seconds to timedelta
            if 'time_span' in repo_stats and isinstance(repo_stats['time_span'], (int, float)):
                repo_stats['time_span'] = datetime.timedelta(seconds=repo_stats['time_span'])
        
        # Convert dates in aggregate stats
        for key in ['first_commit', 'last_commit']:
            if key in stats['aggregate_stats'] and isinstance(stats['aggregate_stats'][key], str):
                try:
                    stats['aggregate_stats'][key] = parse(stats['aggregate_stats'][key])
                except:
                    pass
                
        # Convert time_span from seconds to timedelta
        if 'time_span' in stats['aggregate_stats'] and isinstance(stats['aggregate_stats']['time_span'], (int, float)):
            stats['aggregate_stats']['time_span'] = datetime.timedelta(seconds=stats['aggregate_stats']['time_span'])
    
    def process_stats(self):
        """Process statistics and extract deeper insights."""
        result = {
            'overall': {},
            'repos': {},
            'patterns': {}
        }
        
        # Process aggregate stats
        agg = self.stats['aggregate_stats']
        result['overall'] = {
            'repos_analyzed': agg.get('repos_analyzed', 0),
            'total_commits': agg.get('total_commits', 0),
            'total_branches': agg.get('total_branches', 0),
            'total_lines': agg.get('total_lines', 0)
        }
        
        if 'time_span' in agg and isinstance(agg['time_span'], datetime.timedelta):
            days = agg['time_span'].total_seconds() / (24 * 60 * 60)
            hours = (days % 1) * 24
            minutes = (hours % 1) * 60
            
            result['overall']['time_span'] = {
                'days': int(days),
                'hours': int(hours),
                'minutes': int(minutes),
                'total_minutes': int(agg['time_span'].total_seconds() / 60)
            }
            
            result['overall']['commits_per_day'] = agg.get('total_commits', 0) / max(1, days)
        
        # Process repos stats
        for repo, stats in self.stats['summary_stats'].items():
            result['repos'][repo] = {
                'num_commits': stats.get('num_commits', 0),
                'num_branches': stats.get('num_branches', 0),
                'total_lines': stats.get('total_lines', 0),
                'lines_per_commit': stats.get('lines_per_commit', 0)
            }
            
            if 'time_span' in stats and isinstance(stats['time_span'], datetime.timedelta):
                days = stats['time_span'].total_seconds() / (24 * 60 * 60)
                hours = (days % 1) * 24
                minutes = (hours % 1) * 60
                
                result['repos'][repo]['time_span'] = {
                    'days': int(days),
                    'hours': int(hours),
                    'minutes': int(minutes),
                    'total_minutes': int(stats['time_span'].total_seconds() / 60)
                }
                
                if days > 0:
                    result['repos'][repo]['commits_per_day'] = stats.get('num_commits', 0) / days
            
            # Calculate average time between commits
            if 'avg_time_between_commits' in stats:
                avg_minutes = stats['avg_time_between_commits'] / 60
                result['repos'][repo]['avg_time_between_commits_minutes'] = avg_minutes
            
            # Copy peak pace information
            for key in ['peak_pace_commits', 'peak_pace_duration', 'peak_pace_start', 'peak_pace_end']:
                if key in stats:
                    result['repos'][repo][key] = stats[key]
            
            if 'peak_pace_duration' in stats and 'peak_pace_commits' in stats:
                result['repos'][repo]['peak_pace_minutes_per_commit'] = stats['peak_pace_duration'] / (60 * stats['peak_pace_commits'])
            
            # Extract frequent words from commit messages
            if 'frequent_words' in stats:
                result['repos'][repo]['frequent_words'] = stats['frequent_words']
                
                # Categorize emotional content
                emotional_words = self.categorize_emotional_content(stats['frequent_words'])
                if emotional_words:
                    result['repos'][repo]['emotional_words'] = emotional_words
        
        # Calculate commit patterns
        result['patterns'] = self.analyze_commit_patterns()
        
        self.processed_stats = result
        return result
    
    def categorize_emotional_content(self, word_counts):
        """Categorize words in commit messages into emotional categories."""
        positive_words = ['great', 'nice', 'good', 'awesome', 'love', 'amazing', 'finally', 'works', 'fixed', 'clean']
        negative_words = ['fuck', 'shit', 'bad', 'error', 'bug', 'issue', 'problem', 'fail', 'sucks', 'broken', 'wtf', 'sighs', 'sigh', 'depressed', 'hate', 'waste']
        uncertain_words = ['maybe', 'try', 'attempt', 'hopefully', 'should', 'might', 'could', 'idk', 'unknown', 'testing']
        
        results = {
            'positive': {},
            'negative': {},
            'uncertain': {}
        }
        
        for word, count in word_counts.items():
            word_lower = word.lower()
            
            if word_lower in positive_words:
                results['positive'][word] = count
            elif word_lower in negative_words:
                results['negative'][word] = count
            elif word_lower in uncertain_words:
                results['uncertain'][word] = count
        
        # Only return non-empty categories
        return {k: v for k, v in results.items() if v}
    
    def analyze_commit_patterns(self):
        """Analyze patterns in commit behavior."""
        patterns = {}
        
        # Extract all commits
        all_commits = []
        for repo, commits in self.stats['commit_data'].items():
            for commit in commits:
                all_commits.append({
                    'repo': repo,
                    'date': commit['commit_date'],
                    'message': commit['message']
                })
        
        if not all_commits:
            return patterns
        
        # Convert to DataFrame
        df = pd.DataFrame(all_commits)
        
        # Ensure date is datetime
        if not pd.api.types.is_datetime64_any_dtype(df['date']):
            df['date'] = pd.to_datetime(df['date'])
        
        # Sort by date
        df = df.sort_values('date')
        
        # Calculate time between commits
        df['next_commit'] = df['date'].shift(-1)
        df['time_to_next'] = (df['next_commit'] - df['date']).dt.total_seconds() / 60  # minutes
        
        # Calculate basic statistics
        patterns['time_between_commits'] = {
            'mean_minutes': df['time_to_next'].mean(),
            'median_minutes': df['time_to_next'].median(),
            'min_minutes': df['time_to_next'].min(),
            'max_minutes': df['time_to_next'].max()
        }
        
        # Find bursts of activity
        burst_threshold = 10  # minutes
        df['in_burst'] = df['time_to_next'] < burst_threshold
        
        # Find burst sequences
        df['burst_group'] = (df['in_burst'] != df['in_burst'].shift()).cumsum()
        bursts = df[df['in_burst']].groupby('burst_group').agg({
            'date': 'first',
            'repo': 'first',
            'time_to_next': ['count', 'mean', 'min']
        })
        
        if not bursts.empty:
            # Format burst data
            bursts.columns = ['start_time', 'repo', 'commits', 'avg_minutes', 'min_minutes']
            bursts = bursts.reset_index()
            bursts = bursts.sort_values('commits', ascending=False)
            
            # Extract top bursts
            top_bursts = bursts.head(5).to_dict('records')
            patterns['top_bursts'] = top_bursts
        
        # Analyze time of day patterns
        df['hour'] = df['date'].dt.hour
        hour_counts = df.groupby('hour').size()
        
        patterns['hour_distribution'] = hour_counts.to_dict()
        
        # Find peak hours
        peak_hours = hour_counts.nlargest(3).index.tolist()
        patterns['peak_hours'] = peak_hours
        
        # Analyze day of week patterns
        df['day_of_week'] = df['date'].dt.day_name()
        day_counts = df.groupby('day_of_week').size()
        
        patterns['day_distribution'] = day_counts.to_dict()
        
        # Find peak days
        peak_days = day_counts.nlargest(3).index.tolist()
        patterns['peak_days'] = peak_days
        
        return patterns
    
    def generate_detailed_report(self, output_file="detailed_repo_analysis.md"):
        """Generate a detailed markdown report based on processed statistics."""
        # Make sure stats are processed
        if not self.processed_stats:
            self.process_stats()
        
        stats = self.processed_stats
        
        # Start building the report
        report = []
        
        # Title
        report.append("# Detailed Repository Analysis Report\n")
        report.append(f"Generated on {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # Overall Stats section
        report.append("## Overall Stats\n")
        
        overall = stats['overall']
        report.append(f"- **Repos Analyzed**: {overall['repos_analyzed']}")
        
        if 'time_span' in overall:
            time_span = overall['time_span']
            time_str = f"{time_span['days']} days"
            if time_span['hours'] > 0 or time_span['minutes'] > 0:
                time_str += f", {time_span['hours']} hours"
            if time_span['minutes'] > 0:
                time_str += f", {time_span['minutes']} minutes"
            
            report.append(f"- **Time Span**: {time_str}")
        
        report.append(f"- **Total Commits**: {overall['total_commits']}")
        report.append(f"- **Total Branches**: {overall['total_branches']}")
        report.append(f"- **Total Lines (CTree -1 -vvv)**: {overall['total_lines']}")
        
        if 'commits_per_day' in overall:
            report.append(f"- **Average Commits/Day**: {overall['commits_per_day']:.1f}")
        
        report.append("\n")
        
        # Repo-by-Repo Breakdown
        report.append("## Repo-by-Repo Commit Breakdown\n")
        
        for repo, repo_stats in stats['repos'].items():
            report.append(f"### {repo}\n")
            
            report.append(f"- **Commits**: {repo_stats['num_commits']}")
            
            if 'time_span' in repo_stats:
                time_span = repo_stats['time_span']
                time_str = ""
                if time_span['days'] > 0:
                    time_str += f"{time_span['days']}d, "
                if time_span['hours'] > 0:
                    time_str += f"{time_span['hours']}h, "
                time_str += f"{time_span['minutes']}m"
                
                report.append(f"- **Time Span**: {time_str}")
            
            if 'commits_per_day' in repo_stats:
                report.append(f"- **Commits/Day**: {repo_stats['commits_per_day']:.1f}")
            
            if 'avg_time_between_commits_minutes' in repo_stats:
                avg_minutes = repo_stats['avg_time_between_commits_minutes']
                if avg_minutes < 60:
                    report.append(f"- **Average Time Between Commits**: ~{avg_minutes:.0f} minutes")
                else:
                    hours = avg_minutes / 60
                    report.append(f"- **Average Time Between Commits**: ~{hours:.1f} hours")
            
            if all(k in repo_stats for k in ['peak_pace_commits', 'peak_pace_duration']):
                duration_mins = repo_stats['peak_pace_duration'] / 60
                report.append(f"- **Peak Pace**: {repo_stats['peak_pace_commits']} commits in {duration_mins:.1f}mâ€”~{repo_stats.get('peak_pace_minutes_per_commit', 0):.1f} minutes/commit")
            
            report.append(f"- **Branches**: {repo_stats['num_branches']}")
            
            # Frequent commit message words
            if 'frequent_words' in repo_stats:
                report.append("- **Messages**:")
                
                # Common words
                common_words = [(word, count) for word, count in repo_stats['frequent_words'].items() if count > 1]
                if common_words:
                    common_words = sorted(common_words, key=lambda x: x[1], reverse=True)
                    common_phrases = [f"\"{word}\" ({count}x)" for word, count in common_words[:5]]
                    report.append(f"  - Frequent: {', '.join(common_phrases)}")
                
                # Emotional content
                if 'emotional_words' in repo_stats:
                    for category, words in repo_stats['emotional_words'].items():
                        if words:
                            examples = [f"\"{word}\" ({count}x)" for word, count in list(words.items())[:3]]
                            report.append(f"  - {category.capitalize()}: {', '.join(examples)}")
            
            report.append(f"- **Lines**: {repo_stats['total_lines']}")
            
            report.append("\n")
        
        # Aggregate Stats section
        report.append("## Aggregate Stats\n")
        
        # Total Commits
        report.append("### Total Commits")
        report.append(f"- **{overall['total_commits']} commits** across {time_str}.")
        
        if 'commits_per_day' in overall:
            report.append(f"- **Commits/Day**: ~{overall['commits_per_day']:.1f}")
        
        report.append("\n")
        
        # Commit Frequency
        if 'patterns' in stats and 'time_between_commits' in stats['patterns']:
            report.append("### Commit Frequency")
            time_between = stats['patterns']['time_between_commits']
            
            mean_minutes = time_between['mean_minutes']
            if mean_minutes < 60:
                report.append(f"- **Average Time Between Commits**: ~{mean_minutes:.0f}m")
            else:
                hours = mean_minutes / 60
                report.append(f"- **Average Time Between Commits**: ~{hours:.1f}h")
            
            min_minutes = time_between['min_minutes']
            report.append(f"- **Fastest Pace**: {min_minutes:.0f} seconds/commit")
            
            report.append("\n")
        
        # Peak Days
        if 'patterns' in stats and 'top_bursts' in stats['patterns']:
            report.append("### Peak Activity Bursts")
            
            for i, burst in enumerate(stats['patterns']['top_bursts'][:3]):
                report.append(f"- **{burst['repo']}**: {burst['commits']} commits at ~{burst['avg_minutes']:.1f} min/commit")
            
            report.append("\n")
        
        # Branch Distribution
        report.append("### Branch Distribution")
        report.append(f"- **Total Branches**: {overall['total_branches']}")
        report.append("- **Per Repo**:")
        
        # Sort repos by number of branches
        sorted_repos = sorted([(repo, stats['repos'][repo]['num_branches']) for repo in stats['repos']], 
                             key=lambda x: x[1], reverse=True)
        
        for repo, branches in sorted_repos:
            report.append(f"  - `{repo}`: {branches}")
        
        report.append("\n")
        
        # Lines vs. Commits
        report.append("### Lines vs. Commits")
        
        if overall['total_commits'] > 0:
            overall_lines_per_commit = overall['total_lines'] / overall['total_commits']
            report.append(f"- **Lines/Commit**: ~{overall_lines_per_commit:.0f} ({overall['total_lines']} / {overall['total_commits']})")
        
        # Sort repos by lines per commit
        lines_per_commit = []
        for repo, repo_stats in stats['repos'].items():
            if repo_stats['num_commits'] > 0:
                lines_per_commit.append((repo, repo_stats['total_lines'] / repo_stats['num_commits']))
        
        lines_per_commit = sorted(lines_per_commit, key=lambda x: x[1], reverse=True)
        
        if lines_per_commit:
            report.append(f"- **Highest**: `{lines_per_commit[0][0]}` (~{lines_per_commit[0][1]:.0f} lines/commit)")
            report.append(f"- **Lowest**: `{lines_per_commit[-1][0]}` (~{lines_per_commit[-1][1]:.0f} lines/commit)")
        
        report.append("\n")
        
        # Patterns & Insights
        report.append("## Patterns & Insights\n")
        
        # Sprint Intensity
        report.append("### Sprint Intensity")
        
        # Sort repos by average time between commits
        avg_time_between = []
        for repo, repo_stats in stats['repos'].items():
            if 'avg_time_between_commits_minutes' in repo_stats:
                avg_time_between.append((repo, repo_stats['avg_time_between_commits_minutes']))
        
        avg_time_between = sorted(avg_time_between, key=lambda x: x[1])
        
        if avg_time_between:
            report.append(f"- **Tightest**: `{avg_time_between[0][0]}` ({avg_time_between[0][1]:.1f} min/commit)")
            report.append(f"- **Wildest**: `{avg_time_between[-1][0]}` ({avg_time_between[-1][1]:.1f} min/commit)")
        
        # Find the repo with the most consistent pace
        peak_vs_avg = []
        for repo, repo_stats in stats['repos'].items():
            if 'avg_time_between_commits_minutes' in repo_stats and 'peak_pace_minutes_per_commit' in repo_stats:
                ratio = repo_stats['avg_time_between_commits_minutes'] / repo_stats['peak_pace_minutes_per_commit']
                peak_vs_avg.append((repo, ratio))
        
        if peak_vs_avg:
            most_consistent = min(peak_vs_avg, key=lambda x: abs(x[1] - 1))
            report.append(f"- **Steadiest**: `{most_consistent[0]}` (peak pace close to average)")
        
        report.append("\n")
        
        # Emotional Arc
        if any('emotional_words' in repo_stats for repo_stats in stats['repos'].values()):
            report.append("### Emotional Arc\n")
            
            # Collect emotional words across repos
            struggles = []
            triumphs = []
            fatigue = []
            
            for repo, repo_stats in stats['repos'].items():
                if 'emotional_words' in repo_stats:
                    emotional = repo_stats['emotional_words']
                    
                    if 'negative' in emotional:
                        for word, count in emotional['negative'].items():
                            struggles.append((repo, word, count))
                    
                    if 'positive' in emotional:
                        for word, count in emotional['positive'].items():
                            triumphs.append((repo, word, count))
                    
                    if 'negative' in emotional:
                        fatigue_words = [item for item in emotional['negative'].items() 
                                        if item[0].lower() in ['sighs', 'sigh', 'depressed', 'tired']]
                        for word, count in fatigue_words:
                            fatigue.append((repo, word, count))
            
            # Sort by count
            struggles = sorted(struggles, key=lambda x: x[2], reverse=True)
            triumphs = sorted(triumphs, key=lambda x: x[2], reverse=True)
            fatigue = sorted(fatigue, key=lambda x: x[2], reverse=True)
            
            if struggles:
                examples = [f"\"{word}\" (`{repo}`)" for repo, word, _ in struggles[:3]]
                report.append(f"- **Struggle**: {', '.join(examples)}")
            
            if triumphs:
                examples = [f"\"{word}\" (`{repo}`)" for repo, word, _ in triumphs[:3]]
                report.append(f"- **Triumph**: {', '.join(examples)}")
            
            if fatigue:
                examples = [f"\"{word}\" {count}x (`{repo}`)" for repo, word, count in fatigue[:3]]
                report.append(f"- **Fatigue**: {', '.join(examples)}")
            
            report.append("\n")
        
        # Branching Behavior
        report.append("### Branching Behavior\n")
        
        # Sort repos by number of branches
        sorted_by_branches = sorted([(repo, stats['repos'][repo]['num_branches']) for repo in stats['repos']], 
                                   key=lambda x: x[1], reverse=True)
        
        if sorted_by_branches:
            most_branches = sorted_by_branches[0]
            report.append(f"- **Exploratory**: `{most_branches[0]}` ({most_branches[1]} branches)")
            
            least_branches = sorted_by_branches[-1]
            if least_branches[1] <= 2:
                report.append(f"- **Focused**: `{least_branches[0]}` ({least_branches[1]} branch{'es' if least_branches[1] > 1 else ''})")
            
            # Find middle-ground repos
            if len(sorted_by_branches) > 2:
                middle = sorted_by_branches[len(sorted_by_branches) // 2]
                if middle[1] > 2 and middle[1] < most_branches[1]:
                    report.append(f"- **Balanced**: `{middle[0]}` ({middle[1]} branches)")
        
        report.append("\n")
        
        # Summary Table
        report.append("## Summary Table\n")
        
        # Table header
        headers = ["**Metric**", "**Total/Avg**"]
        for repo, _ in sorted_repos:
            headers.insert(1, f"**{repo}**")
        
        report.append("| " + " | ".join(headers) + " |")
        
        # Separator row
        report.append("| " + " | ".join(["---" for _ in headers]) + " |")
        
        # Metrics rows
        metrics = [
            ("Commits", [stats['repos'][repo]['num_commits'] for repo, _ in sorted_repos], overall['total_commits']),
            ("Lines", [stats['repos'][repo]['total_lines'] for repo, _ in sorted_repos], overall['total_lines']),
            ("Branches", [stats['repos'][repo]['num_branches'] for repo, _ in sorted_repos], overall['total_branches'])
        ]
        
        for metric_name, repo_values, total in metrics:
            row = [f"**{metric_name}**"]
            for value in repo_values:
                row.insert(1, str(value))
            row.append(str(total))
            report.append("| " + " | ".join(row) + " |")
        
        # Time Span row
        time_spans = []
        for repo, _ in sorted_repos:
            if 'time_span' in stats['repos'][repo]:
                time_span = stats['repos'][repo]['time_span']
                time_str = f"{time_span['days']}d {time_span['hours']}h {time_span['minutes']}m"
                time_spans.append(time_str)
            else:
                time_spans.append("N/A")
        
        if 'time_span' in overall:
            overall_time = overall['time_span']
            overall_time_str = f"{overall_time['days']}d {overall_time['hours']}h {overall_time['minutes']}m"
        else:
            overall_time_str = "N/A"
        
        row = ["**Time Span**"]
        for time_str in time_spans:
            row.insert(1, time_str)
        row.append(overall_time_str)
        report.append("| " + " | ".join(row) + " |")
        
        # Avg Time/Commit row
        avg_times = []
        for repo, _ in sorted_repos:
            if 'avg_time_between_commits_minutes' in stats['repos'][repo]:
                minutes = stats['repos'][repo]['avg_time_between_commits_minutes']
                if minutes < 60:
                    avg_times.append(f"{minutes:.0f}m")
                else:
                    hours = minutes / 60
                    avg_times.append(f"{hours:.1f}h")
            else:
                avg_times.append("N/A")
        
        if 'patterns' in stats and 'time_between_commits' in stats['patterns']:
            mean_minutes = stats['patterns']['time_between_commits']['mean_minutes']
            if mean_minutes < 60:
                overall_avg = f"{mean_minutes:.0f}m"
            else:
                hours = mean_minutes / 60
                overall_avg = f"{hours:.1f}h"
        else:
            overall_avg = "N/A"
        
        row = ["**Avg Time/Commit**"]
        for time_str in avg_times:
            row.insert(1, time_str)
        row.append(overall_avg)
        report.append("| " + " | ".join(row) + " |")
        
        # Peak Pace row
        peak_paces = []
        for repo, _ in sorted_repos:
            if 'peak_pace_minutes_per_commit' in stats['repos'][repo]:
                minutes = stats['repos'][repo]['peak_pace_minutes_per_commit']
                peak_paces.append(f"{minutes:.1f}m")
            else:
                peak_paces.append("N/A")
        
        if 'patterns' in stats and 'time_between_commits' in stats['patterns']:
            best_pace = stats['patterns']['time_between_commits']['min_minutes']
            overall_best = f"{best_pace:.1f}m"
        else:
            overall_best = "N/A"
        
        row = ["**Peak Pace**"]
        for pace in peak_paces:
            row.insert(1, pace)
        row.append(overall_best)
        report.append("| " + " | ".join(row) + " |")
        
        # Commits/Day row
        commits_per_day = []
        for repo, _ in sorted_repos:
            if 'commits_per_day' in stats['repos'][repo]:
                commits_per_day.append(f"{stats['repos'][repo]['commits_per_day']:.1f}")
            else:
                commits_per_day.append("N/A")
        
        if 'commits_per_day' in overall:
            overall_cpd = f"{overall['commits_per_day']:.1f}"
        else:
            overall_cpd = "N/A"
        
        row = ["**Commits/Day**"]
        for cpd in commits_per_day:
            row.insert(1, cpd)
        row.append(overall_cpd)
        report.append("| " + " | ".join(row) + " |")
        
        # Write the report
        with open(output_file, 'w') as f:
            f.write('\n'.join(report))
        
        return output_file
    
    def generate_visualizations(self, output_dir="."):
        """Generate additional visualizations based on processed stats."""
        # Make sure stats are processed
        if not self.processed_stats:
            self.process_stats()
        
        stats = self.processed_stats
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        visualizations = {}
        
        # 1. Commits by Repository - Pie Chart
        repos = list(stats['repos'].keys())
        commits = [stats['repos'][repo]['num_commits'] for repo in repos]
        
        plt.figure(figsize=(10, 8))
        plt.pie(commits, labels=repos, autopct='%1.1f%%', startangle=90, shadow=True)
        plt.title('Commits by Repository')
        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
        
        pie_chart_path = os.path.join(output_dir, "commits_by_repo_pie.png")
        plt.savefig(pie_chart_path)
        plt.close()
        
        visualizations['commits_pie'] = pie_chart_path
        
        # 2. Lines of Code by Repository - Bar Chart
        repos = list(stats['repos'].keys())
        lines = [stats['repos'][repo]['total_lines'] for repo in repos]
        
        plt.figure(figsize=(12, 8))
        bars = plt.bar(repos, lines, color='skyblue')
        
        # Add the text labels
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height):,}',
                    ha='center', va='bottom', rotation=0)
        
        plt.title('Lines of Code by Repository')
        plt.xlabel('Repository')
        plt.ylabel('Lines of Code')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        lines_chart_path = os.path.join(output_dir, "lines_by_repo.png")
        plt.savefig(lines_chart_path)
        plt.close()
        
        visualizations['lines_bar'] = lines_chart_path
        
        # 3. Commits over Time - Line Chart
        # Extract all commits
        all_commits = []
        for repo, commits in self.stats['commit_data'].items():
            for commit in commits:
                all_commits.append({
                    'repo': repo,
                    'date': commit['commit_date']
                })
        
        if all_commits:
            df = pd.DataFrame(all_commits)
            
            # Ensure date is datetime
            if not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'])
            
            # Group by date and repository
            df['day'] = df['date'].dt.floor('D')
            commits_by_day = df.groupby(['day', 'repo']).size().reset_index(name='count')
            
            # Pivot for plotting
            pivot = commits_by_day.pivot(index='day', columns='repo', values='count').fillna(0)
            
            # Create stacked area chart
            plt.figure(figsize=(15, 8))
            pivot.plot.area(stacked=True, alpha=0.7, figsize=(15, 8))
            
            plt.title('Commits Over Time by Repository')
            plt.xlabel('Date')
            plt.ylabel('Number of Commits')
            plt.legend(title='Repository')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            commits_time_path = os.path.join(output_dir, "commits_over_time.png")
            plt.savefig(commits_time_path)
            plt.close()
            
            visualizations['commits_time'] = commits_time_path
        
        # 4. Commit Velocity - Histogram
        all_commit_times = []
        for repo, commits in self.stats['commit_data'].items():
            # Sort commits by date
            sorted_commits = sorted(commits, key=lambda x: x['commit_date'])
            
            # Calculate time differences
            for i in range(len(sorted_commits) - 1):
                time_diff = (sorted_commits[i+1]['commit_date'] - sorted_commits[i]['commit_date']).total_seconds() / 60
                all_commit_times.append({
                    'repo': repo,
                    'time_diff_minutes': time_diff
                })
        
        if all_commit_times:
            df = pd.DataFrame(all_commit_times)
            
            # Filter out extreme values for better visualization
            df = df[df['time_diff_minutes'] <= 120]  # Only show gaps up to 2 hours
            
            plt.figure(figsize=(12, 8))
            sns.histplot(data=df, x='time_diff_minutes', hue='repo', bins=24, kde=True, element='step')
            
            plt.title('Distribution of Time Between Commits (up to 2 hours)')
            plt.xlabel('Minutes Between Commits')
            plt.ylabel('Frequency')
            plt.tight_layout()
            
            time_dist_path = os.path.join(output_dir, "commit_time_distribution.png")
            plt.savefig(time_dist_path)
            plt.close()
            
            visualizations['time_distribution'] = time_dist_path
        
        # 5. Branches by Repository - Bar Chart
        repos = list(stats['repos'].keys())
        branches = [stats['repos'][repo]['num_branches'] for repo in repos]
        
        plt.figure(figsize=(12, 8))
        bars = plt.bar(repos, branches, color='lightgreen')
        
        # Add the text labels
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}',
                    ha='center', va='bottom', rotation=0)
        
        plt.title('Number of Branches by Repository')
        plt.xlabel('Repository')
        plt.ylabel('Number of Branches')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        branches_chart_path = os.path.join(output_dir, "branches_by_repo.png")
        plt.savefig(branches_chart_path)
        plt.close()
        
        visualizations['branches_bar'] = branches_chart_path
        
        # 6. Heat Map of Commit Activity by Hour and Day
        # Extract all commits
        all_commits = []
        for repo, commits in self.stats['commit_data'].items():
            for commit in commits:
                all_commits.append({
                    'repo': repo,
                    'date': commit['commit_date']
                })
        
        if all_commits:
            df = pd.DataFrame(all_commits)
            
            # Ensure date is datetime
            if not pd.api.types.is_datetime64_any_dtype(df['date']):
                df['date'] = pd.to_datetime(df['date'])
            
            # Extract hour and day of week
            df['hour'] = df['date'].dt.hour
            df['day_of_week'] = df['date'].dt.day_name()
            
            # Ensure day of week is ordered properly
            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
            df['day_of_week'] = pd.Categorical(df['day_of_week'], categories=day_order, ordered=True)
            
            # Group by hour and day of week
            heatmap_data = df.groupby(['day_of_week', 'hour']).size().unstack().fillna(0)
            
            # Plot heatmap
            plt.figure(figsize=(15, 10))
            sns.heatmap(heatmap_data, cmap='viridis', annot=True, fmt='g')
            
            plt.title('Commit Activity by Hour and Day of Week')
            plt.xlabel('Hour of Day')
            plt.ylabel('Day of Week')
            plt.tight_layout()
            
            heatmap_path = os.path.join(output_dir, "commit_heatmap.png")
            plt.savefig(heatmap_path)
            plt.close()
            
            visualizations['heatmap'] = heatmap_path
        
        return visualizations


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate detailed statistics from repository analysis."
    )
    parser.add_argument(
        '--stats',
        help='Path to the repository stats JSON file',
        required=True,
    )
    parser.add_argument(
        '--output',
        help='Output directory for reports and visualizations',
        default='./repo_stats',
    )
    
    return parser.parse_args()


def main():
    """Main entry point for the script."""
    args = parse_arguments()
    
    print(f"Starting repository statistics generation from {args.stats}")
    
    # Create the stats generator
    generator = StatGenerator(args.stats)
    
    # Process the stats
    generator.process_stats()
    
    # Generate detailed report
    report = generator.generate_detailed_report(os.path.join(args.output, "detailed_repo_analysis.md"))
    print(f"Generated detailed report: {report}")
    
    # Generate visualizations
    visualizations = generator.generate_visualizations(args.output)
    print("Generated visualizations:")
    for name, path in visualizations.items():
        print(f"- {name}: {path}")
    
    print("\nDone!")


if __name__ == "__main__":
    main()
